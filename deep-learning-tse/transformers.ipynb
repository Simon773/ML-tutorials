{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "The goal of this module is to understand the **Transformer architecture** from its core component: the **Self-Attention mechanism**. Grasping this architecture is essential for comprehending state-of-the-art models in Natural Language Processing (NLP), including the Large Language Models (LLMs) driving recent advances.\n",
    "\n",
    "### Text Processing with Tokens\n",
    "\n",
    "Before a Transformer model can process any text, the raw string must be converted into a numerical sequence of inputs that the model understands. This process is called **tokenization**, and the resulting numerical units are **tokens**.\n",
    "\n",
    "A **token** is the fundamental unit of text: it is the smallest block of text that is passed to a model. Tokens serve two purposes:\n",
    "1.  **Semantic Unit:** They represent meaningful chunks of language, such as words, punctuation, or parts of a word.\n",
    "2.  **Numerical ID:** Each unique token is mapped to a unique integer ID, which is then used to look up its corresponding **embedding vector** (the dense numerical representation fed into the Transformer).\n",
    "\n",
    "#### How are Tokens Computed\n",
    "\n",
    "Modern NLP uses sophisticated tokenization methods that strike a balance between vocabulary size and sequence length.\n",
    "\n",
    "- **Word-Level Tokenization:** The simplest approach splits text by spaces, treating each word as a token.\n",
    "    * *Problem:* Leads to a massive vocabulary (e.g., one token for \"run,\" another for \"running,\" another for \"runs\").\n",
    "- **Character-Level Tokenization:** Splits text into individual characters.\n",
    "    * *Problem:* Generates very long input sequences, making the Transformer less efficient.\n",
    "- **Subword Tokenization (The Standard):** This approach used by most LLMs (like GPT or BERT) solves both problems via methods like [**Byte Pair Encoding (BPE)**](https://en.wikipedia.org/wiki/Byte-pair_encoding) or [**WordPiece**](https://huggingface.co/learn/llm-course/en/chapter6/6):\n",
    "    1.  **Splitting:** Common words (like \"the,\" \"is\") are treated as single tokens.\n",
    "    2.  **Breaking Down:** Rare or complex words are broken down into common subword units. For example, \"unbelievable\" might be tokenized as \\[\"un\", \"believ\", \"able\"\\].\n",
    "\n",
    "This subword approach ensures that every word can be represented (even misspelled or rare words) without the vocabulary size exploding, and it keeps sequence lengths manageable.\n",
    "\n",
    "#### Token to Embedding\n",
    "\n",
    "Once the text is tokenized:\n",
    "\n",
    "1.  The sequence of text tokens is converted into a sequence of integer IDs.\n",
    "2.  These IDs are passed to the model's **Embedding Layer**, which acts as a lookup table mapping tokens to $\\mathbf{d}_{model}$-dimensional vectors called embeddings.\n",
    "3.  The layer returns the corresponding sequence of embeddings, which form the **Input Matrix ($\\mathbf{X}$)** for the Self-Attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism\n",
    "\n",
    "**Self-Attention** is a mechanism that allows a model to weigh the importance and relevance of **all other elements** in an input sequence when encoding a single element. It is a powerful way for the model to capture context and dependencies regardless of the distance between tokens in the sequence.\n",
    "\n",
    "##### Input representation\n",
    "\n",
    "The input to the Transformer is a sequence of tokens (words, sub-words, etc.), which is then passed through the embedding layer to get the Input Matrix of shape $(n \\times \\mathbf{d}_{model})$.\n",
    "For each output element $\\mathbf{y}_i$, the model then calculates an **attention score** to determine how much information from $\\mathbf{x}_j$ (where $j=1$ to $n$) should flow into $\\mathbf{y}_i$.\n",
    "\n",
    "\n",
    "##### Query, Key and Value vectors\n",
    "\n",
    "For the attention calculation, the input matrix $\\mathbf{X}$ is projected into three distinct, lower-dimensional representation spaces using three separate learned weight matrices: $\\mathbf{W}^{Q}$, $\\mathbf{W}^{K}$, and $\\mathbf{W}^{V}$.\n",
    "\n",
    "- The **Query** vector $\\mathbf{Q} = \\mathbf{X} \\mathbf{W}^{Q}$ of shape $(\\mathbf{d}_{\\text{model}} \\times d_k)$ is used to query or seek context\n",
    "- The **Key** vector $\\mathbf{K} = \\mathbf{X} \\mathbf{W}^{K}$ of shape: $(\\mathbf{d}_{\\text{model}} \\times d_k)$ (same as the query) is used to label or categorize context\n",
    "- The **Value** vector $\\mathbf{V} = \\mathbf{X} \\mathbf{W}^{V}$ of shape: $(\\mathbf{d}_{\\text{model}} \\times d_v)$ contains the actual information to be passed through the network\n",
    "\n",
    "##### Attention score computation\n",
    "\n",
    "The first step in calculating attention is to determine the **relevance** between every query vector ($\\mathbf{q}_i$) and every key vector ($\\mathbf{k}_j$). This is done via a dot product.\n",
    "\n",
    "The matrix of raw attention scores is computed as:\n",
    "\n",
    "$$\\mathbf{A}_{raw} = \\mathbf{Q} \\mathbf{K}^T$$\n",
    "\n",
    "The scores are then divided by the square root of the key dimension, $\\sqrt{d_k}$. This **scaling** prevents the dot products from growing too large, which would push the softmax function into regions where gradients are extremely small (saturation), hindering stable training.\n",
    "\n",
    "$$\\mathbf{A}_{scaled} = \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "The scaled attention scores are then normalized using the **Softmax** function along the sequence dimension (rows of $\\mathbf{A}_{scaled}$). This converts the scores into a probability distribution, ensuring that the weights for any single token sum to 1.\n",
    "\n",
    "$$\\mathbf{A}_{weights} = \\operatorname{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "The final output is computed by taking the weighted sum of the $\\mathbf{V}$ vectors, using the normalized attention weights ($\\mathbf{A}_{weights}$) as the coefficients.\n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{A}_{weights} \\mathbf{V}$$\n",
    "\n",
    "The resulting matrix $\\mathbf{Z}$ has the shape $(n \\times d_v)$. Each row $\\mathbf{z}_i$ is the new, context-aware representation of the original token $\\mathbf{x}_i$.\n",
    "\n",
    "Combining the steps, the basic **Scaled Dot-Product Attention** formula is:\n",
    "\n",
    "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\operatorname{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_{k}}}\\right) \\mathbf{V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "In practice, to allow the model to focus on different types of relationships simultaneously (e.g., syntactical dependency vs. semantic meaning), the attention calculation is parallelized across $h$ different **heads**.\n",
    "\n",
    "1.  **Multiple Projections:** Instead of one set of $\\mathbf{W}^{Q}, \\mathbf{W}^{K}, \\mathbf{W}^{V}$, we use $h$ sets: $\\mathbf{W}^{Q}_i, \\mathbf{W}^{K}_i, \\mathbf{W}^{V}_i$ for $i=1$ to $h$.\n",
    "2.  **Parallel Attention:** Each head $\\text{head}_i$ computes its own attention output $\\mathbf{Z}_i$.\n",
    "3.  **Concatenation & Final Projection:** The outputs from all $h$ heads are concatenated back together and then passed through a final learned linear transformation ($\\mathbf{W}^{O}$) to project the result back into the required output dimension.\n",
    "\n",
    "The formula for the $i$-th head is:\n",
    "$$\\text{head}_i = \\text { Attention }(\\mathbf{X} \\mathbf{W}_{i}^{Q}, \\mathbf{X} \\mathbf{W}_{i}^{K}, \\mathbf{X} \\mathbf{W}_{i}^{V})$$\n",
    "\n",
    "The final **Multi-Head Attention** output is:\n",
    "$MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})= \\operatorname{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\mathbf{W}^{O$\n",
    "\n",
    "<p style=\\\"text-align:center\\\">\n",
    "    <img src=\"https://wikidocs.net/images/page/159310/mha_img_original.png\" alt=\"Multi-head attention mechanism\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Transformer Architecture: Encoder and Decoder\n",
    "\n",
    "The complete Transformer architecture consists of stacked layers of Self-Attention mechanisms and Feed-Forward networks. These layers are organized into two main components: the **Encoder** and the **Decoder**.\n",
    "\n",
    "### Encoder (Contextualization)\n",
    "\n",
    "The **Encoder** is responsible for taking the input sequence (e.g., a sentence) and generating a rich, contextual representation for **every token**.\n",
    "It consists of a stack of identical layers, each containing a **Multi-Head Self-Attention** sub-layer and a Feed-Forward sub-layer.\n",
    "\n",
    "The Encoder processes the input text **bidirectionally**, allowing each token to attend to *all* other tokens (past and future) in the input sequence.\n",
    "\n",
    "### Decoder (Generation)\n",
    "\n",
    "The **Decoder** is responsible for taking the contextualized representation from the Encoder and generating the output sequence one token at a time.\n",
    "Each Decoder layer has three sub-layers: a **Masked Multi-Head Self-Attention** sub-layer, a **Cross-Attention** sub-layer, and a Feed-Forward sub-layer.\n",
    "\n",
    "The Masked Self-Attention ensures the Decoder is **unidirectional** (auto-regressive), meaning it can only attend to tokens it has **already generated** (or the start-of-sequence token). In practice, this is achieved by setting the scaled attention scores to $-\\infty$ before applying the softmax to force the attention to $0$ on future tokens.\n",
    "\n",
    "The **Cross-Attention** sub-layer allows the decoding process to selectively focus on the most relevant parts of the **entire input sequence** that was generally processed by the Encoder.\n",
    "\n",
    "Cross-Attention uses the same core attention formula, $\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})$, but draws its three components from **two different sources**: the Query vector comes from the output of the previous Masked Self-Attention layer of the Decoder, whereas the Key and Value vectors come from the final output of the entire Encoder stack.\n",
    "The output is a new context vector for the Decoder that is highly relevant to both the previously generated sequence and the original input sequence.\n",
    "\n",
    "### Different Architectures for Different Tasks\n",
    "\n",
    "The need for both an Encoder and a Decoder depends entirely on the task type:\n",
    "- **Encoder-Only Models** (like BERT) are used when the goal is to fully understand a given input sequence (e.g., classifying a movie review as positive/negative). Since no new text is generated, the Decoder is unnecessary.\n",
    "- **Decoder-Only Models** (like GPT) are used for **Large Language Models** and generative tasks. They are trained to predict the next token in a sequence based only on the preceding tokens, making the full Encoder redundant. They rely entirely on their Masked Self-Attention to build context.\n",
    "- **Encoder-Decoder Models** are reserved for tasks like Machine Translation, where a source sequence (Encoder) is read, and a distinct target sequence (Decoder) is generated.\n",
    "\n",
    "\n",
    "### References\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Self-Attention in NLP](https://www.youtube.com/watch?v=5vcj8kSwBCY)\n",
    "\n",
    "<p style=\\\"text-align:center\\\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" alt=\"Multi-head attention mechanism\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the tranformers, you will use this following tensor as example\n",
    "dummy_tensor = torch.randn((16, 32, 64)) # A tensor of shape (batch_size, sequence_length, embedding_size)\n",
    "\n",
    "# you can imagine a batch of 16 sentence of 32 tokens, each token being represented by a 64-dimensional embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.1: </b>\n",
    "* Implement the self-attention mechanism\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module): \n",
    "    \"\"\"\n",
    "    Implements a single head of the Scaled Dot-Product Self-Attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, dim_head: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.scale_factor = dim_head ** -0.5  # Scaling factor: 1 / sqrt(d_k)\n",
    "\n",
    "        # Q, K, V projections are linear layers (weights W^Q, W^K, W^V)\n",
    "        self.query = nn.Linear(input_dim, dim_head, bias=False)\n",
    "        self.key = nn.Linear(input_dim, dim_head, bias=False)\n",
    "        self.value = nn.Linear(input_dim, dim_head, bias=False)\n",
    "\n",
    "        # Softmax is applied on the last dimension (the key/sequence dimension)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequence: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        sequence: tensor of shape (batch_size, sequence_length, input_dim)\n",
    "        attention_mask: tensor of shape (batch_size, sequence_length)\n",
    "        \"\"\"\n",
    "        # Compute Q, K and V\n",
    "        Q = ...\n",
    "        K = ...\n",
    "        V = ...\n",
    "\n",
    "        # Compute the scaled attention scores Q x K^T\n",
    "        raw_score = ...\n",
    "        scaled_score = raw_score * self.scale_factor\n",
    "        \n",
    "        # Apply Mask if provided\n",
    "        if attention_mask is not None:\n",
    "            # The mask needs to be broadcastable to the attention score tensor (B, N, N)\n",
    "            # The input mask is (B, N), we need to expand it to (B, 1, N) to mask the last dim\n",
    "            mask = attention_mask.unsqueeze(1).bool()\n",
    "            mask_value = torch.finfo(dots.dtype).min  # smallest possible number, acts as -inf\n",
    "            scaled_score.masked_fill_(~mask, mask_value) \n",
    "\n",
    "        # Apply Softmax\n",
    "        attention_score = ...\n",
    "\n",
    "        # Compute head output\n",
    "        head = ...\n",
    "\n",
    "        return head, attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer = SelfAttention(input_dim=64, dim_head=16)\n",
    "head, attention_score = self_attention_layer(dummy_tensor)\n",
    "print(head.shape, attention_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.2: </b>\n",
    "* Implement the multihead attention mechanism\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads  # total dimension after combining all heads\n",
    "        self.heads = heads\n",
    "        self.scale_factor = dim_head ** -0.5  # Scaling factor: 1 / sqrt(d_k)\n",
    "\n",
    "        # Projections W^Q, W^K, W^V are combined into single layers of size (dim -> inner_dim)\n",
    "        # We split the heads later via reshaping.\n",
    "        self.query = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.key = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.value = nn.Linear(dim, inner_dim, bias=False)\n",
    "\n",
    "        # Final output projection W^O\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        h = self.heads\n",
    "        B, N, D = x.shape  # Batch, Sequence length, Model dimension\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = ...\n",
    "        K = ...\n",
    "        V = ...\n",
    "        \n",
    "        # Split Heads and Transpose\n",
    "        # Reshape to (B, N, heads, dim_head) -> Transpose to (B, heads, N, dim_head)\n",
    "        # This makes the attention calculation treat the 'heads' dimension independently\n",
    "        Q = Q.reshape(B, N, h, -1).transpose(1, 2)\n",
    "        K = K.reshape(B, N, h, -1).transpose(1, 2)\n",
    "        V = V.reshape(B, N, h, -1).transpose(1, 2)\n",
    "\n",
    "        # Compute the scaled attention scores (Q @ K^T)\n",
    "        scaled_score = ...\n",
    "\n",
    "        # Apply Mask\n",
    "        if mask is not None:\n",
    "            # The mask must be broadcastable to (B, heads, N, N)\n",
    "            # From a (B, N) mask, we unsqueeze twice to get (B, 1, 1, N)\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2).bool()\n",
    "            mask_value = torch.finfo(attention_scores.dtype).min\n",
    "            scaled_score.masked_fill_(~mask, mask_value)\n",
    "\n",
    "        # Apply Softmax\n",
    "        attention_score = ...\n",
    "\n",
    "        # Compute head output\n",
    "        out = ...\n",
    "\n",
    "        # Final linear projection W^O\n",
    "        out = ...\n",
    "\n",
    "        return out, attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention_layer = MultiHeadAttention(64, 8, 64)\n",
    "out, attention_scores = multihead_attention_layer(dummy_tensor)\n",
    "print(out.shape, attention_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.3: </b>\n",
    "* Implement the tranformer encoder block\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single block of the Transformer Encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.mha = MultiHeadAttention(dim, heads, dim_head)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        # Multi-Head Attention Sub-layer\n",
    "        attention_output, _ = ...\n",
    "        x = x + attention_output  # Residual connection\n",
    "\n",
    "        # Feed-Forward (MLP) Sub-layer\n",
    "        mlp_output = ...\n",
    "        x = x + mlp_output  # Residual connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_block = TransformerEncoderBlock(64, 8, 64)\n",
    "out = encoder_block(dummy_tensor)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> (Optional) Exercise 3.4: </b>\n",
    "* Implement the tranformer decoder block\n",
    "* Combine the encoder and decoder to create a transformer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(MultiHeadAttention):\n",
    "    \"\"\"\n",
    "    Simulates the Cross-Attention layer by inheriting MHA structure.\n",
    "    It takes the encoder output (z) to generate K and V, and the decoder \n",
    "    output (x) to generate Q.\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor, z: torch.Tensor, mask: Optional[torch.Tensor] = None) :\n",
    "        B, N_x, D = x.shape\n",
    "        h = self.heads\n",
    "\n",
    "        # Q is computed from the Decoder's output (x)\n",
    "        Q = ...\n",
    "        # K and V are computed from the Encoder's output (z)\n",
    "        K = ...\n",
    "        V = ...\n",
    "\n",
    "        N_z = z.shape[1]\n",
    "        Q = Q.reshape(B, N_x, h, -1).transpose(1, 2)\n",
    "        K = K.reshape(B, N_z, h, -1).transpose(1, 2)\n",
    "        V = V.reshape(B, N_z, h, -1).transpose(1, 2)\n",
    "\n",
    "        scaled_score = ...\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2).bool()\n",
    "            mask_value = torch.finfo(attention_scores.dtype).min\n",
    "            scaled_score.masked_fill_(~mask, mask_value)\n",
    "        \n",
    "        attention_scores = ...\n",
    "        out = ...\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.ln3 = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.masked_mha = MultiHeadAttention(dim, heads, dim_head) \n",
    "        self.cross_attn = CrossAttention(dim, heads, dim_head) \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, z: torch.Tensor, self_attn_mask: Optional[torch.Tensor] = None, cross_attn_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        x: Decoder input (output from previous decoder layer)\n",
    "        z: Encoder output (output of the final encoder block)\n",
    "        self_attn_mask: Mask for self-attention (causal/look-ahead mask)\n",
    "        cross_attn_mask: Mask for cross-attention (e.g., padding mask from encoder input)\n",
    "        \"\"\"\n",
    "        attention_output, _ = ...\n",
    "        x = x + attention_output\n",
    "        \n",
    "        cross_output = ...\n",
    "        x = x + cross_output\n",
    "        \n",
    "        mlp_output = self.mlp(self.ln3(x))\n",
    "        x = x + mlp_output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_block = TransformerEncoderBlock(64, 8, 64)\n",
    "z = encoder_block(dummy_tensor)\n",
    "decoder_block = TransformerDecoderBlock(64, 8, 64)\n",
    "out = decoder_block(dummy_tensor, z)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The full Encoder-Decoder Transformer architecture.\n",
    "    \n",
    "    This class stacks the Encoder and Decoder blocks to perform sequence-to-sequence tasks.\n",
    "    It takes an input sequence (src) and a target sequence (tgt) and uses the Encoder\n",
    "    output (z) as the K/V source for the Decoder's cross-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 512, depth: int = 6, heads: int = 8, dim_head: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_stack = nn.ModuleList([\n",
    "            TransformerEncoderBlock(dim=dim, heads=heads, dim_head=dim_head)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_stack = nn.ModuleList([\n",
    "            TransformerDecoderBlock(dim=dim, heads=heads, dim_head=dim_head)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.final_ln = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        src: torch.Tensor, \n",
    "        tgt: torch.Tensor, \n",
    "        src_mask: Optional[torch.Tensor] = None, \n",
    "        tgt_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source sequence (e.g., input sentence). Shape (B, N_src, D).\n",
    "            tgt: Target sequence (e.g., partially generated output). Shape (B, N_tgt, D).\n",
    "            src_mask: Padding mask for the source sequence (used in encoder and cross-attention).\n",
    "            tgt_mask: Causal mask for the target sequence (used in decoder self-attention).\n",
    "\n",
    "        Returns:\n",
    "            The final output representation from the decoder. Shape (B, N_tgt, D).\n",
    "        \"\"\"\n",
    "        output = ...\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LLM is just a stack of transformer encoder and decoder layers with usually a huge number of parameters and a wide feedforward dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT marked a major turning point in NLP as it introduced a novel pre-training strategy that allowed language models to capture deep, bidirectional context, significantly boosting performance across nearly all downstream tasks.\n",
    "\n",
    "##### What is the Core Purpose of BERT?\n",
    "\n",
    "The purpose of BERT is to create a **general-purpose \"language understanding\" model** that is highly effective at encoding the meaning of words based on their full context. It achieves this by being **pre-trained** in an **unsupervised** manner on a massive, general-purpose text corpus (like Wikipedia and BooksCorpus).\n",
    "\n",
    "The resulting model weights serve as a powerful foundation that can be quickly adapted to specialized tasks.\n",
    "\n",
    "A **downstream task** refers to a specific, practical NLP application (e.g., question answering, sentiment analysis, named entity recognition...).\n",
    "\n",
    "**Fine-Tuning** is the process of taking BERT's pre-trained weights and continuing to train the entire model (or just the final layers) on a small, labeled dataset specific to the downstream task. This adaptation allows the model to specialize its general language understanding for the target task, which is far more efficient than training a model from scratch.\n",
    "\n",
    "### Why BERT Models Outperformed Previous Methods\n",
    "\n",
    "BERT's superiority comes from its **deep bidirectionality**, a key architectural feature enabled by the Transformer Encoder.\n",
    "Older models like OpenAI GPT-1 (using Transformer Decoders) or LSTMs were often **unidirectional** (left-to-right). Consider the sentence:   `I made a bank deposit`\n",
    "\n",
    "In a unidirectional model, when calculating the representation for the word \"**bank**,\" the model only looks at the preceding words (\"I,\" \"made,\" \"a\") but ignores the succeeding word \"**deposit**.\" It misses the full context, which is crucial for determining the correct meaning of \"bank\" (e.g., financial institution vs. river bank).\n",
    "\n",
    "BERT is composed entirely of the **Transformer Encoder** stack. Because Encoder layers use **non-masked Self-Attention**, every token's representation is computed using information from **all other tokens** in the input sequence, from the very first layer up to the last. This makes BERT **deeply bidirectional**.\n",
    "\n",
    "### How BERT is Pre-Trained\n",
    "\n",
    "BERT is trained on two **unsupervised** tasks simultaneously on its massive text corpus. The combined training loss is the sum of the loss from both tasks.\n",
    "\n",
    "#### Task 1: Masked Language Modeling (MLM)\n",
    "\n",
    "The goal of MLM is to force the model to capture the **bidirectional context** of a word by predicting words that have been intentionally hidden.\n",
    "\n",
    "1.  **Masking:** Approximately **15%** of the tokens in the input sentence are selected as prediction targets.\n",
    "2.  **Prediction:** The model must predict the identity of the masked tokens based on the surrounding, unmasked context.\n",
    "\n",
    "```python\n",
    "input = 'the man went to the [MASK1] . he bought a [MASK2] of milk'\n",
    "label = {'[MASK1]': 'store', '[MASK2]': 'gallon'}\n",
    "```\n",
    "\n",
    "**Masking Strategy (The Trick)**: To prevent the model from always knowing a word has been hidden, the $15\\%$ of selected tokens are modified as follows:\n",
    "\n",
    "  * **80% of the time:** Replace the word with the special [MASK] token (the standard case).\n",
    "  * **10% of the time:** Replace the word with a random word (forcing the model to decide if the word fits the context).\n",
    "  * **10% of the time:** Keep the word unchanged (forcing the model to keep generating contextual representations for every token).\n",
    "\n",
    "#### Task 2: Next Sentence Prediction (NSP)\n",
    "\n",
    "NSP is designed to give BERT an understanding of the **relationship between two distinct sentences**, which is essential for tasks like Question Answering or Document Classification.\n",
    "\n",
    "1.  **Input Structure:** Two sentences, Sentence A and Sentence B, are concatenated and separated by the \\[SEP\\] token.\n",
    "2.  **Task:** The model must classify whether Sentence B is the actual next sentence that follows Sentence A in the corpus (labeled \"**IsNext**\") or if it is a random, unrelated sentence (labeled \"**NotNext**\").\n",
    "\n",
    "$$\\text{[CLS] The rain stopped pouring [SEP] The sun started to shine [SEP]} \\quad \\Rightarrow \\text{IsNext}$$\n",
    "$$\\text{[CLS] The capital of France is Paris [SEP] I am going to buy a cat [SEP]} \\quad \\Rightarrow \\text{NotNext}$$\n",
    "\n",
    "BERT was trained on vast, document-level corpora:\n",
    "\n",
    "  * BooksCorpus ($\\approx 800\\text{M}$ words)\n",
    "  * English Wikipedia ($\\approx 2,500\\text{M}$ words)\n",
    "\n",
    "It is crucial to use **document-level corpora** rather than collections of shuffled, unrelated sentences, as this is the only way to effectively train the Next Sentence Prediction task and teach the model about long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "Whereas BERT is trained to understand natural language and contextually encode text, models like GPT (3, 4, 5) and LLaMA are trained to generate new tokens.\n",
    "\n",
    "These models are composed of a Decoder-only stack, and use masked Self-Attention layers to compute token's representation based on the information from only the previous tokens in the input sequence (**unidirectionality**).\n",
    "\n",
    "They're trained on a vast amount of public internet data on a **Causal Language Modeling (CLM)** task, i.e. predicting the next token of a sequence.\n",
    "\n",
    "BERT is composed entirely of the Transformer Encoder stack. Because Encoder layers use non-masked Self-Attention, every token's representation is computed using information from all other tokens in the input sequence, from the very first layer up to the last. This makes BERT deeply bidirectional.\n",
    "\n",
    "#### The Secret Ingredient for Modern GPT Models: RLHF\n",
    "\n",
    "To give the GPT family models their human-like conversational ability, they're fine-tuned after pre-training _via_ a method called **Reinforcement Learning with Human Feedback (RLHF)**.\n",
    "\n",
    "RLHF uses human preferences to train a separate \"reward model,\" which then guides the GPT model to generate responses that are helpful, truthful, and harmless, making the output feel much more aligned with human dialogue and intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT for text classification\n",
    "\n",
    "In this section, we'll fine-tune the BERT model on a simple task of text classification consisting in predicting the number of stars (between 0 and 4 corresponding to 1 star to 5 stars) assigned to a Yelp review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Yelp/yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nb of tokens:\", tokenizer.vocab_size)\n",
    "print({k:v for k, v in list(tokenizer.get_vocab().items())[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Hello world! Welcome to the TSE Machine Learning course.\"\n",
    "ids = tokenizer(test_sentence)[\"input_ids\"]\n",
    "print(\"Tokens:\", ids)\n",
    "print(\"//\".join(map(tokenizer.decode, ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_train_dataset)\n",
    "print(small_train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.5: </b>\n",
    "* Using the [`transformers` library](https://huggingface.co/docs/transformers/index), download the BERT uncased model (name \"bert-base-uncased\") and change the number of labels to 5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.6: </b>\n",
    "* Make an inference with the model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the first example\n",
    "example = small_train_dataset[0]\n",
    "\n",
    "# Extract the input fields required by the model\n",
    "input_ids = ...\n",
    "attention_mask = ...\n",
    "\n",
    "# Convert to the format expected by the model i.e. (batch_size, sequence_length)\n",
    "inputs = {\n",
    "    \"input_ids\": ...,\n",
    "    \"attention_mask\": ...,\n",
    "}\n",
    "\n",
    "\n",
    "# Get predictions from the model\n",
    "with torch.no_grad():\n",
    "    outputs = ...\n",
    "\n",
    "# The outputs are logits, you can apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    nb_iteration = 0\n",
    "    total_data = 0\n",
    "    for batch in dataloader:\n",
    "        # Move batch to the same device as model\n",
    "        y = batch.pop(\"label\").to(device)\n",
    "        text = batch.pop(\"text\")\n",
    "\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        input_data = {k: batch[k].to(device) for k in (\"input_ids\", \"attention_mask\")}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_data)\n",
    "        logits = outputs.logits\n",
    "        total_data += logits.size()[0]  # Get the batch size\n",
    "        loss = loss_function(logits, y)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == y)\n",
    "        nb_iteration += 1\n",
    "\n",
    "    avg_loss = total_eval_loss / nb_iteration\n",
    "    accuracy = correct_predictions.double() / total_data\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.7: </b>\n",
    "* Add the AdamW optimizer with a Learning Rate $\\alpha = 5\\mathrm{e}{-5}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import ...\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Ensure the model is on the correct device (GPU or CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = ...\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.8: </b>\n",
    "* Perform the training\n",
    "* Why the performance is so bad?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # for displaying a progress bar\n",
    "\n",
    "# Define the number of training epochs\n",
    "epochs = 3\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    total_loss = 0\n",
    "    nb_iteration = 0\n",
    "\n",
    "    train_iter = small_train_dataset.iter(batch_size=8)\n",
    "    eval_iter = small_eval_dataset.iter(batch_size=8)\n",
    "\n",
    "    for batch in tqdm(train_iter, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\"):\n",
    "        # Move batch to the same device as model\n",
    "        y = batch.pop(\"label\").to(device)\n",
    "        text = batch.pop(\"text\")\n",
    "        \n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        input_data = {k: batch[k].to(device) for k in (\"input_ids\", \"attention_mask\")}\n",
    "        outputs = model(**input_data)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs.logits, y)\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters and zero the gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        nb_iteration += 1\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss over an epoch\n",
    "    avg_train_loss = total_loss / nb_iteration\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} complete! Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    eval_loss, eval_accuracy = evaluate_model(model, eval_iter, loss_function, device)\n",
    "    print(f\"Validation Loss: {eval_loss:.4f}, Validation Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.9: </b>\n",
    "* Play with the training procedure to reach the best possible performance: add a learning rate scheduler, increase the training dataset size etc.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
