{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab1e36f-4983-4df0-8dd4-fa848f504dca",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "In computing, an image is fundamentally represented as a **multi-dimensional tensor (array)** of pixel intensity values. The typical dimensionality for a color image is $(C, H, W)$, where:\n",
    "\n",
    "* **C (Channels):** The depth dimension, representing the different color components of a pixel. For standard RGB (Red, Green, Blue) images, $C=3$.\n",
    "* **H (Height):** The number of pixels along the vertical axis.\n",
    "* **W (Width):** The number of pixels along the horizontal axis.\n",
    "\n",
    "For example, a standard color image with a resolution of $1080 \\times 1920$ is represented as a tensor of shape $\\text{3} \\times \\text{1080} \\times \\text{1920}$.\n",
    "\n",
    "While images are structured data, the task of **Computer Vision**, that is enabling machines to \"see\" and interpret the visual world, remains one of the most challenging problems in AI. A slight shift in viewpoint, lighting, or background can drastically change the raw pixel values of an image, yet a human can instantly recognize the underlying object.\n",
    "\n",
    "Deep Learning, particularly through the use of **Convolutional Neural Networks (CNNs)**, provides the necessary framework to learn robust, hierarchical, and translation-invariant features directly from these pixel tensors, transforming raw visual data into meaningful semantic information.\n",
    "In this notebook, we'll introduce the main ideas behind CNNs, and train our own network on a simple image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e956d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 2.0: </b>\n",
    "* Pick your favorite image and load it using the [`PIL.Image` library](https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.open).\n",
    "* Convert the image to a grayscale image (cf. [here](https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.convert)).\n",
    "* Convert the image to a numpy array.\n",
    "* Display the image using `matplotlib.pyplot`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed66eb3b-75f3-4787-a4da-ebccb00d3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load image\n",
    "image = ...\n",
    "\n",
    "# Convert it to grayscale and to a numpy array\n",
    "image = ...\n",
    "\n",
    "# Display it using plt\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b605cac-d542-4a1b-ab0d-65024c9ec3a4",
   "metadata": {},
   "source": [
    "## Convolutions: The Foundation of Image Processing\n",
    "\n",
    "Convolution is the fundamental operation in modern Computer Vision, particularly within Convolutional Neural Networks (CNNs). It is a linear operation applied across the pixel values of an input image tensor.\n",
    "\n",
    "The convolution is defined by a kernel (or filter), which is a small, multi-dimensional array of learnable parameters (weights).\n",
    "The kernel is systematically slid (convolved) across the entire input image, pixel by pixel.\n",
    "At each position, the convolution layer performs an element-wise multiplication between the kernel's values and the corresponding patch of input pixels, followed by a summation (a dot product). This result becomes a single pixel value in the output feature map.\n",
    "Since the kernel is typically much smaller than the input image, this operation is highly efficient compared to a full matrix multiplication across the entire image.\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif\" alt=\"Dot Product gif available at: https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071e1f8-21e3-40d4-a062-9c43d1c5ef81",
   "metadata": {},
   "source": [
    "## Properties of Convolutions\n",
    "\n",
    "The power of convolutions stems from two crucial properties:\n",
    "\n",
    "1. **Translation Invariance** (or Equivariance):\n",
    "This property means that applying the same kernel across the entire image allows the network to learn a feature (e.g., an edge, a curve) that can be detected regardless of its location in the image. If a feature is recognized in the top-left corner, the same kernel can recognize it in the bottom-right corner. This is essential for object detection and recognition.\n",
    "\n",
    "2. **Locality** (Sparse Interaction):\n",
    "The output value of a pixel in the feature map depends only on the values of the pixels in its immediate neighborhood (the area covered by the kernel). This property exploits the fact that features in natural images are formed by local, spatial correlations, significantly reducing the number of parameters the network needs to learn compared to a fully connected layer.\n",
    "\n",
    "The output of a convolution operation is a new feature map whose size is determined by the input size, the kernel size, and other hyperparameters like stride and padding.\n",
    "\n",
    "**Padding** involves strategically adding a border of values (typically zeros) around the input image tensor before the convolution is applied.\n",
    "The primary purpose is to allow the kernel to cover the edge pixels of the input. Without padding, the output size shrinks with every convolution. By adding padding, we can ensure the output feature map has the same spatial dimensions as the input, a configuration often called \"same padding.\"\n",
    "\n",
    "The **Stride** parameter specifies the number of pixels the kernel shifts across the input image after each computation.\n",
    "With S = 1 (default), the kernel moves one pixel at a time, resulting in an output feature map size very similar to the input (depending on padding).\n",
    "With S > 1, the kernel skips pixels, causing the output feature map to be spatially down-sampled (reduced in size). This is a common way to reduce computation and memory use in the network's early layers.\n",
    "\n",
    "Padding: 1 Striding: 1 | Padding: 2 Striding: 1 | Padding: 1 Striding: 2\n",
    ":---------------------:|:----------------------:|:----------------------:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/Convolution_arithmetic_-_Arbitrary_padding_no_strides_transposed.gif/640px-Convolution_arithmetic_-_Arbitrary_padding_no_strides_transposed.gif) | ![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Convolution_arithmetic_-_Arbitrary_padding_no_strides.gif/640px-Convolution_arithmetic_-_Arbitrary_padding_no_strides.gif) | ![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Convolution_arithmetic_-_Padding_strides.gif/640px-Convolution_arithmetic_-_Padding_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c32cb0-dee8-4eef-beab-7d4726bbbf4e",
   "metadata": {},
   "source": [
    "Another way of modifying the output size of the convolution is the use of pooling layers.\n",
    "**Pooling** (or Subsampling) is a non-linear operation, usually inserted periodically between successive convolution layers, designed to progressively reduce the spatial size of the representation. This operation allows for reducing the number of parameters in the network (as the output of the layer is smaller than its input thus decreasing the computational load for subsequent layers) and enhancing robustness as it provides a form of translation invariance by making the network less sensitive to small shifts and distortions in the input.\n",
    "\n",
    "The most frequently used pooling method is **Max Pooling**: given a defined window size (e.g., 2×2) and a stride, the operation slides the window over the input feature map and outputs the maximum value within that window.\n",
    "\n",
    "This technique aggressively reduces the spatial size (e.g., a 2×2 pool with stride 2 halves the width and height) while retaining the most salient features (the largest activation values) within the local region.\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/640px-Max_pooling.png\" alt=\"Max pooling img available at: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/640px-Max_pooling.png\" />\n",
    "</p>\n",
    "\n",
    "If the functioning of CNN is still unclear, you can take a look at this [article](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) or this [Stanford course](https://cs231n.github.io/convolutional-networks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b2401-751e-43a7-94f2-805c0b768b78",
   "metadata": {},
   "source": [
    "The examples above show 2D images and 2D kernels, but since it is a dot product it can also be applied with RGB images (or on any number of channels, e.g. hyperspectral images):\n",
    "\n",
    "2D | 3D\n",
    ":---: | :---:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/a5/Convolutional_Neural_Network_NeuralNetworkFilter.gif?20190217151520) | ![](https://upload.wikimedia.org/wikipedia/commons/9/95/Convolutional_Neural_Network_with_Color_Image_Filter.gif?20190217151516)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be2b70",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 2.1: </b>\n",
    "* Implement your convolution function using `numpy`.\n",
    "* Apply your convolution function to the grayscale image from exercise 1.0. With a kernel of size 3x3, and with the following values: `[[1, 0, -1], [1, 0, -1], [1, 0, -1]]`.\n",
    "* Display the output image using `matplotlib.pyplot`.\n",
    "* What do you think this convolution kernel is doing to the image?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_convolution(image: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n",
    "    # Your function should handle padding\n",
    "    ...\n",
    "\n",
    "\n",
    "kernel = ...\n",
    "convoluted_image = my_convolution(image, kernel)\n",
    "\n",
    "plt.imshow(convoluted_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f612c5-662d-4734-83ee-bab09f031982",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "While the conceptual definition of convolution involves sliding a kernel, high-performance computing often requires more optimized approaches. Techniques like the [*im2col trick*](https://hackmd.io/@machine-learning/blog-post-cnnumpy-fast) (Image-to-Column) or Fast Fourier Transform (FFT) are utilized in hardware-accelerated libraries to reorganize the tensor data. This allows the convolution to be computed as an efficient matrix multiplication, leveraging highly optimized linear algebra routines.\n",
    "\n",
    "As seen in the previous module, data in PyTorch is stored in Tensors. For Computer Vision, Tensors are used to store image data (typically in $(N, C, H, W)$ format, where $N$ is the batch size).\n",
    "\n",
    "PyTorch provides built-in utilities (like the torchvision package) to access common datasets (e.g., CIFAR, MNIST) and tools (Dataset and DataLoader) for efficiently loading and batching large amounts of image data into memory on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cccb9-b41a-4008-ac2f-4f9d364c67a8",
   "metadata": {},
   "source": [
    "Let's take a look at FashionMNIST, a dataset of 60,000 28x28 grayscale images split in 10 classes of 6,000 images each (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle Boot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e013d",
   "metadata": {
    "id": "e45e013d",
    "outputId": "c88d0f82-6786-4352-87c2-39d66bd4705c"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data = datasets.FashionMNIST(root=\"data\", download=True, transform=ToTensor())\n",
    "# ToTensor converts a PIL image or NumPy ndarray into a FloatTensor\n",
    "# and scales the image's pixel intensity values in the range [0., 1.]\n",
    "\n",
    "dataloader = DataLoader(data, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a1c85",
   "metadata": {
    "id": "7f6a1c85"
   },
   "source": [
    "Dataloaders are iterables: they don't load the whole dataset in memory but only the minibatches on which you want to perform operations.  \n",
    "They behave like Iterators in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4dec3",
   "metadata": {
    "id": "d4d4dec3"
   },
   "outputs": [],
   "source": [
    "X, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264824d",
   "metadata": {
    "id": "0264824d"
   },
   "source": [
    "Unlike TensorFlow, in Pytorch the axis of the channels is located on the second dimension (the first being the batch dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d7037",
   "metadata": {
    "id": "536d7037",
    "outputId": "08c238f2-2f67-4213-90cb-6b1ba7277273"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5de9d0",
   "metadata": {
    "id": "ce5de9d0"
   },
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"t-shirt\",\n",
    "    1: \"trouser\",\n",
    "    2: \"pullover\",\n",
    "    3: \"dress\",\n",
    "    4: \"coat\",\n",
    "    5: \"sandal\",\n",
    "    6: \"shirt\",\n",
    "    7: \"sneaker\",\n",
    "    8: \"bag\",\n",
    "    9: \"ankle boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be967348",
   "metadata": {
    "id": "be967348",
    "outputId": "83b87bf3-b221-4e5f-d985-29fd1ba7a407"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[y[i].item()])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(X[i].permute(1, 2, 0), cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c0779-4317-4e93-8ee3-05bc05488dd4",
   "metadata": {},
   "source": [
    "# Defining Neural Networks in PyTorch\n",
    "\n",
    "In PyTorch, all neural network layers and complete models inherit from the base class `torch.nn.Module`. This framework provides the essential structure for defining trainable models.\n",
    "To create a custom neural network, you typically define a new class that inherits from `nn.Module`. This class requires two key methods:\n",
    "\n",
    "* `__init__(self, ...)`: The constructor is used to initialize all the individual layers, components, and sub-modules that the network will use (e.g., Convolution layers, Linear layers, Batch Normalization). These components become the trainable parameters of the model.\n",
    "\n",
    "* `forward(self, x)`: This method defines the sequence of operations (the computational graph) that transforms the input tensor $x$ into the network's output $\\hat{y}$. It describes how the initialized layers are connected and what operations are performed.\n",
    "\n",
    "A key benefit of using the nn.Module is that you do not need to implement the backward pass manually.\n",
    "When the forward pass is executed, PyTorch automatically records the sequence of operations.\n",
    "The backward() method is then automatically generated using Autograd (PyTorch's automatic differentiation engine), which applies the chain rule to compute all necessary gradients $\\frac{\\partial L}{\\partial \\mathbf{W}}$.\n",
    "\n",
    "In the following exercises, we will implement these concepts by training a simple neural network to classify fashion items using the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfb621",
   "metadata": {
    "id": "05dfb621"
   },
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9116e99d-bdee-4917-b4a1-5d444994a413",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 2.2: </b>\n",
    "* Write a convolutional neural network that takes as input a 28x28 image and outputs a vector of size 10.\n",
    "\n",
    "The model should look like this:\n",
    "```\n",
    "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (relu): ReLU()\n",
    "  (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (relu): ReLU()\n",
    "  (pooling2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    "  (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
    "  (relu): ReLU()\n",
    "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
    "  (relu): ReLU()\n",
    "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
    "```\n",
    "**Hint**: all these layers are defined in the `torch.nn` module.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b804d0a2-e70d-400f-a147-923314dcd276",
   "metadata": {},
   "source": [
    "### Note: Regularization using Dropout\n",
    "\n",
    "As we quickly mentioned at the end of the previous notebook, **Dropout** is a powerful and widely used regularization technique designed to prevent neural networks from overfitting to the training data.\n",
    "During training, at every forward pass of a layer with dropout enabled, each neuron (or feature map element, in Dropout2d) is temporarily \"dropped out\" (set to zero) with a given probability p (e.g., p=0.2 here).\n",
    "This creates a different, \"thinned\" network for every training sample. During evaluation, Dropout is disabled.\n",
    "\n",
    "By randomly dropping neurons, the network cannot rely on any single neuron or specific set of neurons to make a prediction, which forces other neurons to step up and learn redundant, more robust features.\n",
    "\n",
    "Dropout can be viewed as training an exponentially large number of different \"thinned\" networks that share weights. When the full network is used during testing, it effectively acts as an ensemble of all these thinned networks, which significantly reduces the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c8ac9",
   "metadata": {
    "id": "279c8ac9"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FashionMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # write the model \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the computation graph\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5d659",
   "metadata": {
    "id": "7fd5d659",
    "outputId": "8b7ec6d3-516a-4f07-ebca-c50884589c34"
   },
   "outputs": [],
   "source": [
    "model = FashionMNISTClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e140cf0",
   "metadata": {
    "id": "1e140cf0"
   },
   "source": [
    "A second way to create networks is to build a sequence of modules using an instance of the `Sequential` class.  \n",
    "This class will simply create a single branch graph that will call all its elements sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c30378",
   "metadata": {
    "id": "a8c30378",
    "outputId": "da5114ef-4408-4263-910b-5617e4c84c33"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=32 * 4 * 4, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877d807",
   "metadata": {
    "id": "b877d807"
   },
   "source": [
    "You can easily mix the two ways for better readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23ebbf",
   "metadata": {
    "id": "ef23ebbf",
    "outputId": "278280cf-2931-4df7-ec10-50bf521d9016"
   },
   "outputs": [],
   "source": [
    "class FashionMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.classifier = # write the second part using nn.Sequential\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        y = self.classifier(x)\n",
    "        return y\n",
    "    \n",
    "model = FashionMNISTClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85d00c",
   "metadata": {
    "id": "2c85d00c"
   },
   "source": [
    "Pytorch allows to train neural networks using the acceleration provided by GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6884690",
   "metadata": {
    "id": "f6884690"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e80951c-30e0-4652-aaa6-1b5d830dc2ef",
   "metadata": {
    "id": "bdc0d782"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a962c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 2.3: </b>\n",
    "* Complete the training loop below.\n",
    "* Comment each line\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f75143",
   "metadata": {
    "id": "a3f75143"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "def train(net, optimizer, loader, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc='Epoch', leave=False):\n",
    "        running_loss = []\n",
    "        t = tqdm(loader, desc='batch', leave=False)\n",
    "        for x, y in t:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_loss.append(loss.item())\n",
    "            ...\n",
    "            ...\n",
    "            ...\n",
    "            t.set_description(f'Training loss: {mean(running_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19ea91",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "ad19ea91",
    "outputId": "ed76e65a-5f40-4002-aba7-bf77ddb15318"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "train(model, optimizer, train_dataloader, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466d4d2",
   "metadata": {
    "id": "9466d4d2"
   },
   "source": [
    "When we want to evaluate our network (i.e. compute the forward pass only), it is no longer necessary to store the information allowing us to compute the gradients.  \n",
    "In order to speed up the inference, Pytorch offers an option to avoid these unnecessary computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc972a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 2.4: </b>\n",
    "* Explain what is `torch.no_grad()`.\n",
    "* What does the `test()` function return?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ba467",
   "metadata": {
    "id": "649ba467"
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    test_corrects = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x).argmax(1)\n",
    "            test_corrects += y_hat.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return test_corrects / total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b93ddec-cde3-4525-affa-0f8b5be46a35",
   "metadata": {
    "id": "5d4a11e3"
   },
   "source": [
    "As mentionned above, certain layers like Dropout (or Batch-normalization) have a different behavior during training and evaluation. It it therefore important to indicate the learning mode to Pytorch using `model.train()` or `model.eval()` (cf. [documentation](https://docs.pytorch.org/docs/stable/notes/autograd.html#evaluation-mode-nn-module-eval))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf467b",
   "metadata": {
    "id": "82cf467b",
    "outputId": "6463e376-f8c5-44d2-e2e3-ed50bc7a7fa6"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_acc = test(model, test_dataloader)\n",
    "print(f'Test accuracy: {100 * test_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24fac4-efa9-4285-949d-ce74a06b81e2",
   "metadata": {},
   "source": [
    "## Saving and Loading Model Weights\n",
    "\n",
    "In PyTorch, the trainable parameters (weights and biases) of an `nn.Module` are stored in an internal dictionary called the state dictionary (`state_dict`). It is a mapping of every layer to its corresponding parameter tensor (cf. [documentation](https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)).\n",
    "\n",
    "To save your trained model to disk, you can use the [`torch.save` method](https://docs.pytorch.org/docs/stable/generated/torch.save.html) to serialize this state dictionary.\n",
    "\n",
    "To restore a model's state, you must first create an instance of the model class (which defines the architecture) and then load the saved parameters using the [`load_state_dict` method](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665f31c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 2.5: </b>\n",
    "* Save the trained model on disk, then load it again and perform an evaluation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a468a-3132-46b7-87bf-affcee82dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in \"data/fashion_mnist_classifier.pth\"\n",
    "...\n",
    "\n",
    "# Delete the model from memory\n",
    "del model\n",
    "\n",
    "# Recreate the model and load its weights from the saved file\n",
    "model = ...\n",
    "\n",
    "# Re-evaluate it\n",
    "model.eval()\n",
    "test_acc = test(model, test_dataloader)\n",
    "print(f'Test accuracy: {100 * test_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4fa6f-6aa4-4ad9-b4cb-91d16e8dabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    x_i, y_i = X[i].unsqueeze(0), y[i]\n",
    "    y_pred = model(x_i).argmax(1)\n",
    "    true_label = labels_map[y_i.item()]\n",
    "    pred_label = labels_map[y_pred.item()]\n",
    "    plt.title(f\"P:{pred_label} ({true_label})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(X[i].permute(1, 2, 0), cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
